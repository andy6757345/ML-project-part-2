# accuracy 0.62361
# voting classifier with trained svm 
import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load training dataset
train_df = pd.read_csv('train.csv')

# Drop irrelevant columns and handle missing values
train_df.drop(columns=['trip_ID', 'special_requirements'], inplace=True)
train_df.dropna(subset=['category'], inplace=True)

# Impute missing values for 'female_count' and 'male_count'
train_df['female_count'] = train_df['female_count'].fillna(train_df['female_count'].median())
train_df['male_count'] = train_df['male_count'].fillna(train_df['male_count'].median())

# Feature Engineering: create additional useful features
train_df['total_people'] = train_df['female_count'] + train_df['male_count']
train_df['female_to_male_ratio'] = train_df['female_count'] / (train_df['male_count'] + 1)
train_df['male_to_female_ratio'] = train_df['male_count'] / (train_df['female_count'] + 1)

# Categorical processing with one-hot encoding
categorical_cols = train_df.select_dtypes(include=['object']).columns
train_df[categorical_cols] = train_df[categorical_cols].fillna('Unknown')
train_df = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)

# Prepare features (X) and target (y)
X = train_df.drop(columns=['category'])
y = train_df['category']

# Standardize numerical features
numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns
scaler = StandardScaler()
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

# Train the SVM model separately
svm_model = SVC(probability=True, kernel='linear', C=1, random_state=42)
svm_model.fit(X, y)

# Initialize other models for the Voting Classifier
logistic_model = LogisticRegression(max_iter=1000, random_state=42)
decision_tree = DecisionTreeClassifier(random_state=42, max_depth=10)

# Ensemble: Voting Classifier with trained SVM
voting_classifier = VotingClassifier(
    estimators=[
        ('trained_svm', svm_model),  # Use the pre-trained SVM
        ('log_reg', logistic_model),
        ('dt', decision_tree)
    ],
    voting='soft'  # 'soft' uses probabilities for better performance
)

# Fit the ensemble model
voting_classifier.fit(X, y)

# Load test dataset for predictions
test_df = pd.read_csv('test.csv')
test_df.drop(columns=['trip_ID', 'special_requirements'], inplace=True)
test_df['female_count'] = test_df['female_count'].fillna(test_df['female_count'].median())
test_df['male_count'] = test_df['male_count'].fillna(test_df['male_count'].median())

# Feature Engineering for test data
test_df['total_people'] = test_df['female_count'] + test_df['male_count']
test_df['female_to_male_ratio'] = test_df['female_count'] / (test_df['male_count'] + 1)
test_df['male_to_female_ratio'] = test_df['male_count'] / (test_df['female_count'] + 1)

# Categorical processing with one-hot encoding for test data
test_df[categorical_cols] = test_df[categorical_cols].fillna('Unknown')
test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)

# Ensure the test set columns match the training set
test_df = test_df.reindex(columns=X.columns, fill_value=0)
test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])

# Predictions on test data
test_predictions = voting_classifier.predict(test_df)

# Prepare the output
output = pd.DataFrame({
    'trip_ID': pd.read_csv('test.csv')['trip_ID'],
    'category': test_predictions
})

# Save predictions to CSV
output.to_csv('voting_classifier_with_trained_svm_predictions.csv', index=False)
print("Predictions saved to 'voting_classifier_with_trained_svm_predictions.csv'.")
