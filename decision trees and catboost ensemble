# accuracy 0.65071
import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from catboost import CatBoostClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV

# Load training dataset
train_df = pd.read_csv('train.csv')

# Drop irrelevant columns and handle missing values
train_df.drop(columns=['trip_ID', 'special_requirements'], inplace=True)
train_df.dropna(subset=['category'], inplace=True)

# Impute missing values for 'female_count' and 'male_count'
train_df['female_count'] = train_df['female_count'].fillna(train_df['female_count'].median())
train_df['male_count'] = train_df['male_count'].fillna(train_df['male_count'].median())

# Feature Engineering: create additional useful features
train_df['total_people'] = train_df['female_count'] + train_df['male_count']
train_df['female_to_male_ratio'] = train_df['female_count'] / (train_df['male_count'] + 1)
train_df['male_to_female_ratio'] = train_df['male_count'] / (train_df['female_count'] + 1)

# Categorical processing with one-hot encoding
categorical_cols = train_df.select_dtypes(include=['object']).columns
train_df[categorical_cols] = train_df[categorical_cols].fillna('Unknown')
train_df = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)

# Prepare features (X) and target (y)
X = train_df.drop(columns=['category'])
y = train_df['category']

# Standardize numerical features
numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns
scaler = StandardScaler()
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

# Train-test split for validation purposes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
decision_tree = DecisionTreeClassifier(random_state=42)
catboost_model = CatBoostClassifier(iterations=200, learning_rate=0.1, depth=6, silent=True, random_seed=42)

# Grid Search for Hyperparameter Tuning
dt_param_grid = {
    'max_depth': [5, 10, 15, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

catboost_param_grid = {
    'iterations': [100, 200, 300],
    'depth': [4, 6, 8],
    'learning_rate': [0.05, 0.1, 0.2]
}

# GridSearchCV for Decision Tree
grid_search_dt = GridSearchCV(estimator=decision_tree, param_grid=dt_param_grid, cv=3, n_jobs=-1, verbose=1)
grid_search_dt.fit(X_train, y_train)

# GridSearchCV for CatBoost
grid_search_cb = GridSearchCV(estimator=catboost_model, param_grid=catboost_param_grid, cv=3, n_jobs=-1, verbose=1)
grid_search_cb.fit(X_train, y_train)

# Best parameters found
print(f"Best parameters for Decision Tree: {grid_search_dt.best_params_}")
print(f"Best parameters for CatBoost: {grid_search_cb.best_params_}")

# Update models with best parameters
best_dt = grid_search_dt.best_estimator_
best_cb = grid_search_cb.best_estimator_

# Ensemble: Voting Classifier
ensemble_model = VotingClassifier(
    estimators=[('dt', best_dt), ('catboost', best_cb)],
    voting='soft'
)

# Fit the ensemble model
ensemble_model.fit(X_train, y_train)

# Predictions on the test data
y_pred_ensemble = ensemble_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred_ensemble)
print(f"Ensemble Model Accuracy: {accuracy}")
print("Classification Report:\n", classification_report(y_test, y_pred_ensemble))

# Load test dataset for predictions
test_df = pd.read_csv('test.csv')
test_df.drop(columns=['trip_ID', 'special_requirements'], inplace=True)
test_df['female_count'] = test_df['female_count'].fillna(test_df['female_count'].median())
test_df['male_count'] = test_df['male_count'].fillna(test_df['male_count'].median())

# Feature Engineering for test data
test_df['total_people'] = test_df['female_count'] + test_df['male_count']
test_df['female_to_male_ratio'] = test_df['female_count'] / (test_df['male_count'] + 1)
test_df['male_to_female_ratio'] = test_df['male_count'] / (test_df['female_count'] + 1)

# Categorical processing with one-hot encoding for test data
test_df[categorical_cols] = test_df[categorical_cols].fillna('Unknown')
test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)

# Ensure the test set columns match the training set
test_df = test_df.reindex(columns=X.columns, fill_value=0)
test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])

# Predictions on test data
test_predictions = ensemble_model.predict(test_df)

# Prepare the output
output = pd.DataFrame({
    'trip_ID': pd.read_csv('test.csv')['trip_ID'],
    'category': test_predictions
})

# Save predictions to CSV
output.to_csv('ensemble_predictions99.csv', index=False)
print("Predictions saved to 'ensemble99.csv'.")
