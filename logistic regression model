# 0.69487 accuracy 
# logistic regression model
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE, BorderlineSMOTE
from sklearn.utils.class_weight import compute_class_weight

# Load training dataset
train_df = pd.read_csv('train.csv')

# Drop irrelevant columns and handle missing values
train_df.drop(columns=['trip_ID', 'special_requirements'], inplace=True)
train_df.dropna(subset=['category'], inplace=True)

# Impute missing values for 'female_count' and 'male_count'
train_df['female_count'] = train_df['female_count'].fillna(train_df['female_count'].median())
train_df['male_count'] = train_df['male_count'].fillna(train_df['male_count'].median())

# Feature Engineering: create additional useful features
train_df['total_people'] = train_df['female_count'] + train_df['male_count']
train_df['female_to_male_ratio'] = train_df['female_count'] / (train_df['male_count'] + 1)
train_df['male_to_female_ratio'] = train_df['male_count'] / (train_df['female_count'] + 1)

# Adding interactions between features
train_df['people_ratio'] = train_df['total_people'] / (train_df['female_count'] + 1)

# Categorical processing with one-hot encoding
categorical_cols = train_df.select_dtypes(include=['object']).columns
train_df[categorical_cols] = train_df[categorical_cols].fillna('Unknown')
train_df = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)

# Prepare features (X) and target (y)
X = train_df.drop(columns=['category'])
y = train_df['category']

# Standardize numerical features
numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns
scaler = StandardScaler()
X[numerical_cols] = scaler.fit_transform(X[numerical_cols])

# Handle class imbalance with advanced SMOTE (e.g., BorderlineSMOTE)
smote = BorderlineSMOTE(random_state=42)  # BorderlineSMOTE may handle class imbalance better
X_res, y_res = smote.fit_resample(X, y)

# Compute class weights for Logistic Regression (important if the classes are still imbalanced)
class_weights = compute_class_weight('balanced', classes=np.unique(y_res), y=y_res)
class_weight_dict = dict(zip(np.unique(y_res), class_weights))

# Initialize Logistic Regression model with higher max_iter and regularization
log_reg = LogisticRegression(class_weight=class_weight_dict, max_iter=1000, random_state=42, solver='liblinear', C=0.5)

# Fit the Logistic Regression model directly (no validation)
log_reg.fit(X_res, y_res)

# Make predictions on the resampled data
y_pred_log_reg = log_reg.predict(X_res)

# Evaluate the model
train_accuracy_log_reg = accuracy_score(y_res, y_pred_log_reg)
print(f"Logistic Regression Training Accuracy: {train_accuracy_log_reg}")
print("Logistic Regression Classification Report:\n", classification_report(y_res, y_pred_log_reg))

# Prepare the test data and make predictions
test_df = pd.read_csv('test.csv')
test_df.drop(columns=['trip_ID', 'special_requirements'], inplace=True)
test_df['female_count'] = test_df['female_count'].fillna(test_df['female_count'].median())
test_df['male_count'] = test_df['male_count'].fillna(test_df['male_count'].median())

# Feature Engineering for test data
test_df['total_people'] = test_df['female_count'] + test_df['male_count']
test_df['female_to_male_ratio'] = test_df['female_count'] / (test_df['male_count'] + 1)
test_df['male_to_female_ratio'] = test_df['male_count'] / (test_df['female_count'] + 1)
test_df['people_ratio'] = test_df['total_people'] / (test_df['female_count'] + 1)

# Categorical processing with one-hot encoding for test data
test_df[categorical_cols] = test_df[categorical_cols].fillna('Unknown')
test_df = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)

# Ensure the test set columns match the training set
test_df = test_df.reindex(columns=X.columns, fill_value=0)
test_df[numerical_cols] = scaler.transform(test_df[numerical_cols])

# Predictions on test data using the trained Logistic Regression model
pred_probs = log_reg.predict_proba(test_df)
results = pd.DataFrame({
    'trip_ID': pd.read_csv('test.csv')['trip_ID'],
    'category': np.argmax(pred_probs, axis=1)
})

# Save predictions
results.to_csv('predictions_log_reg_optimized_v2.csv', index=False)
